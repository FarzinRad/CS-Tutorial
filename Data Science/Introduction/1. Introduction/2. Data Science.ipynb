{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fatal-notebook",
   "metadata": {},
   "source": [
    "## Data Science\n",
    "\n",
    "Data analytics has been described by Eric Schmidt, the former CEO of Google, as the Future of Everything. For more information, check out a YouTube video called Why Data Analytics is the Future of Everything at https://www.youtube.com/watch?v=9hDnO_ykC7Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-observation",
   "metadata": {},
   "source": [
    "The volume and velocity of data will continue to increase in the big data age. Companies that can efficiently collect, filter, and analyze data that results in information that allows them to better meet the needs of their customers in a much quicker timeframe will gain a significant advantage over their competitors. For example, data analytics (the Culture of Metrics) plays a very key role in the business strategy of Amazon. For more information, refer to the Amazon.com case study by Smart Insights at http://bit.ly/1glnA1u."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-stockholm",
   "metadata": {},
   "source": [
    "## Data analytics pipeline\n",
    "\n",
    "Data modeling is the process of using data to build predictive models. Data can also be used for descriptive and prescriptive analysis. But before we make use of data, it has to be fetched from several sources, stored, assimilated, cleaned, and engineered to suit our goal. The sequential operations that need to be performed on data are akin to a manufacturing pipeline, where each subsequent step adds value to the potential end product and each progression requires a new person or skill set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-soccer",
   "metadata": {},
   "source": [
    "The various steps in a data analytics pipeline are shown in the following diagram: \n",
    "\n",
    "<img src=\"../images/ds-pipe.png\" alt=\"ds-steps\" width=600 align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-perception",
   "metadata": {},
   "source": [
    "These steps can be combined into three high-level categories:\n",
    "1. Data engineering\n",
    "2. Data science\n",
    "3. Product development.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-grounds",
   "metadata": {},
   "source": [
    "**Data Engineering** deals with sourcing data from a variety of sources, creating a suitable database and table schema, and loading the data in a suitable database. There can be many approaches to this step depending on the following:\n",
    "\n",
    "- Type of data: Structured (tabular data) versus unstructured (such as images and text) versus semi-structured (such as JSON and XML)\n",
    "- Velocity of data upgrade: Batch processing versus real-time data streaming\n",
    "- Volume of data: Distributed (or cluster-based) storage versus single instance databases\n",
    "- Variety of data: Document storage, blob storage, or data lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-portrait",
   "metadata": {},
   "source": [
    "**Data Science** is the phase where the data is made usable and used to predict the future, learn patterns, and extrapolate these patterns. Data science can further be sub-divided into two phases.\n",
    "\n",
    "- First phase, wherein the goal is to understand the data better and make it usable. Making the data usable requires considerable effort to clean it by removing invalid characters and missing values. It also involves understanding the nitty-gritty of the data at handâ€”what is the distribution of data, what is the relationship between different data variables, is there a causatory relationship between the input and outcome variable, and so on. It also involves exploring numerical transformations (features) that might explain this causation (between input and outcome variables) better. This phase entails the real forensic effort that goes into the ultimate use of data. To use an analogy, bamboo seeds remain buried in the soil for years with no signs of a sapling growing, and suddenly a sapling grows, and within months a full bamboo tree is ready. This phase of data science is akin to the underground preparation the bamboo seeds undergo before the rapid growth. This is like the stealth mode of a start up wherein a lot of time and effort is committed. And this is where the pandas library, protagonist of this book, finds it raison d'etre and sweet spot.\n",
    "\n",
    "- Second phase where patterns (the parameters of a mathematical expression) are learned from historic data and extrapolated to future data. It involves a lot of experimentation and iterations to get to the optimal results. But if the previous phase have been done with the utmost care, this phase can be implemented pretty quickly thanks to the number of packages in Python, R, and many other data science tools. Of course, it requires a sound understanding of the math and algorithms behind the applied model in order to tweak its parameters to perfection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-unknown",
   "metadata": {},
   "source": [
    "**Product Development** is the phase where all the hard work bears fruit and all the insights, results, and patterns are served to the users in a way that they can consume, understand, and act upon. It might range from building a dashboard on data with additional derived fields to an API that calls a trained model and returns an output on incoming data. A product can also be built to encompass all the stages of the data pipeline, from extracting the data to building a predictive model or creating an interactive dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-paragraph",
   "metadata": {},
   "source": [
    "Apart from these steps in the pipeline, there are some additional steps that might come into the picture. This is due to the highly evolving nature of the data landscape. For example, deep learning, which is used extensively to build intelligent products around image, text, and audio data, often requires the training data to be labeled into a category or augmented if the quantity is too small to create an accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-receptor",
   "metadata": {},
   "source": [
    "For example, an object detection task on video data might require the creation of training data for object boundaries and object classes using some tools, or even manually. Data augmentation helps with image data by creating slightly perturbed data (rotated or grained images, for example) and adding it to training data. For a supervised learning task, labels are mandatory. This label is generally generated together with the data. For example, to train a churn model, a dataset with customer descriptions and when they churned out is required. This information is generally available in the company's CRM tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-philosophy",
   "metadata": {},
   "source": [
    "## Why Python\n",
    "\n",
    "Among the characteristics that make Python popular for data science are its very user-friendly (human-readable) syntax, the fact that it is interpreted rather than compiled (leading to faster development time), and it has very comprehensive libraries for parsing and analyzing data, as well as its capacity for numerical and statistical computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-bishop",
   "metadata": {},
   "source": [
    "Python has libraries that provide a complete toolkit for data science and analysis. The major ones are as follows:\n",
    "\n",
    "- **NumPy**: The general-purpose array functionality with an emphasis on numeric computation\n",
    "- **SciPy**: Numerical computing\n",
    "- **Matplotlib**: Graphics\n",
    "- **pandas**: Series and data frames (1D and 2D array-like types)\n",
    "- **Scikit-learn**: Machine learning\n",
    "- **NLTK**: Natural language processing\n",
    "- **Statstool**: Statistical analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
